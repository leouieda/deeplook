{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "from future.builtins import object, super, range\n",
    "from future.utils import with_metaclass\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import scipy.optimize as spop\n",
    "from fatiando.utils import safe_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NonLinearModel(with_metaclass(ABCMeta)):\n",
    "    def __init__(self, nparams, misfit='l2', optimizer='nelder-mead'):\n",
    "        self.nparams = nparams\n",
    "        self.islinear = False\n",
    "        self.regularization = []\n",
    "        self.misfit = None\n",
    "        self.optimizer = None\n",
    "        self.set_misfit(misfit)\n",
    "        self.set_optimizer(optimizer)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, args):\n",
    "        \"Return data predicted by self.p_\"\n",
    "        pass\n",
    "        \n",
    "    def set_optimizer(self, optimizer):\n",
    "        \"Configure the optimization\"\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_misfit(self, misfit):\n",
    "        \"Pass a different misfit function\"\n",
    "        if misfit == 'l2':\n",
    "            self.misfit = L2NormMisfit()\n",
    "        else:\n",
    "            self.misfit = misfit        \n",
    "        return self\n",
    "    \n",
    "    def add_regularization(self, regul_param, regul):\n",
    "        \"Use the given regularization\"\n",
    "        self.regularization.append([regul_param, regul])\n",
    "        return self\n",
    "    \n",
    "    def _make_partial(self, args, func):\n",
    "        def partial(p):\n",
    "            backup = self.p_\n",
    "            self.p_ = p\n",
    "            res = getattr(self, func)(*args)\n",
    "            self.p_ = backup\n",
    "            return res\n",
    "        return partial\n",
    "            \n",
    "    def fit(self, args, data, weights=None):\n",
    "        \"Fit the model to the given data\"\n",
    "        misfit_args = dict(data=data, predict=self._make_partial(args, 'predict'),\n",
    "                           weights=weights, islinear=self.islinear)\n",
    "        if hasattr(self, 'jacobian'):\n",
    "            misfit_args['jacobian'] = self._make_partial(args, 'jacobian')\n",
    "        misfit = self.misfit(**misfit_args)            \n",
    "        objective = Objective([[1, misfit]] + self.regularization)\n",
    "        self.p_ = self.optimizer.minimize(objective) # the estimated parameter vector\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearModel(NonLinearModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class L2NormMisfit(object):\n",
    "    def __init__(self, data, predict, islinear, jacobian=None, weights=None):\n",
    "        self.data = data\n",
    "        self.predict = predict\n",
    "        self.jacobian = jacobian\n",
    "        self.weights = weights\n",
    "        self.islinear = islinear\n",
    "        self.cache = {'predict': {'hash':None, 'value': None},\n",
    "                      'jacobian': {'hash':None, 'value': None}}\n",
    "        \n",
    "    def from_cache(self, p, func):\n",
    "        if func == 'jacobian' and self.islinear:\n",
    "            if self.cache[func]['value'] is None:\n",
    "                self.cache[func]['value'] = getattr(self, func)(p)\n",
    "        else:\n",
    "            new_hash = hashlib.sha1(p).hexdigest()\n",
    "            old_hash = self.cache[func]['hash']\n",
    "            if old_hash is None or old_hash != new_hash:\n",
    "                self.cache[func]['hash'] = new_hash\n",
    "                self.cache[func]['value'] = getattr(self, func)(p)\n",
    "        return self.cache[func]['value']            \n",
    "    \n",
    "    def value(self, p):\n",
    "        pred = self.from_cache(p, 'predict')\n",
    "        residuals = self.data - pred\n",
    "        if self.weights is None:\n",
    "            return np.linalg.norm(residuals, ord=2)**2\n",
    "        else:\n",
    "            return safe_dot(residuals.T, safe_dot(weights, residuals))\n",
    "        \n",
    "    def gradient(self, p):\n",
    "        jac = self.from_cache(p, 'jacobian')\n",
    "        pred = self.from_cache(p, 'predict')\n",
    "        residuals = self.data - pred\n",
    "        if weights is None:\n",
    "            grad = -2*safe_dot(jac.T, residuals)\n",
    "        else:\n",
    "            grad = -2*safe_dot(jac.T, safe_dot(weights, residuals))\n",
    "        return self._grad_to_1d(grad)\n",
    "    \n",
    "    def _grad_to_1d(self, grad):\n",
    "        # Check if the gradient isn't a one column matrix\n",
    "        if len(grad.shape) > 1:\n",
    "            # Need to convert it to a 1d array so that hell won't break\n",
    "            # loose\n",
    "            grad = np.array(grad).ravel()\n",
    "        return grad\n",
    "    \n",
    "    def gradient_at_null(self):\n",
    "        jac = self.from_cache(p, 'jacobian')\n",
    "        if weights is None:\n",
    "            grad = -2*safe_dot(jac.T, self.data)\n",
    "        else:\n",
    "            grad = -2*safe_dot(jac.T, safe_dot(weights, self.data))\n",
    "        return self._grad_to_1d(grad)        \n",
    "    \n",
    "    def hessian(self, p):\n",
    "        jac = self.from_cache(p, 'jacobian')\n",
    "        if weights is None:\n",
    "            return 2*safe_dot(jac.T, jac)\n",
    "        else:\n",
    "            return 2*safe_dot(jac.T, safe_dot(weights, jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, components):\n",
    "        self.components = components\n",
    "    \n",
    "    def value(self, p):\n",
    "        return np.sum(lamb*comp.value(p) \n",
    "                       for lamb, comp in self.components)\n",
    "    \n",
    "    def gradient(self, p):\n",
    "        return np.sum(lamb*comp.gradient(p) \n",
    "                       for lamb, comp in self.components)\n",
    "    \n",
    "    def hessian(self, p):\n",
    "        return np.sum(lamb*comp.hessian(p) \n",
    "                       for lamb, comp in self.components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussNewton(object):\n",
    "    def __init__(self, initial, tol, precondition):\n",
    "        pass\n",
    "    def minimize(self, objective):\n",
    "        pass\n",
    "    \n",
    "class ScipyOptimizer(object):\n",
    "    def __init__(self, method, **kwargs):\n",
    "        self.method = self._is_valid(method)\n",
    "        self.args = kwargs\n",
    "        \n",
    "    def _is_valid(self, method):\n",
    "        return method\n",
    "    \n",
    "    def minimize(self, objective):\n",
    "        pass    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
